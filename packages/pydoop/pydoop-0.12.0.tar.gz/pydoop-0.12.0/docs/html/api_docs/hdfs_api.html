<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>pydoop.hdfs — HDFS API &mdash; Pydoop 0.12.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.12.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="top" title="Pydoop 0.12.0 documentation" href="../index.html" />
    <link rel="up" title="API Docs" href="index.html" />
    <link rel="next" title="pydoop.utils — Utility Functions" href="utils.html" />
    <link rel="prev" title="pydoop.pipes — MapReduce API" href="mr_api.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="utils.html" title="pydoop.utils — Utility Functions"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="mr_api.html" title="pydoop.pipes — MapReduce API"
             accesskey="P">previous</a> |</li>
	<li><a href="../index.html">Home</a>|&nbsp;</li>
	<li><a href="../installation.html">Download & Install</a>|&nbsp;</li>
	<li><a href="https://sourceforge.net/projects/pydoop/forums/forum/990018">Support</a>|&nbsp;</li>
	<li><a href="http://sourceforge.net/projects/pydoop/">Pydoop on SF</a></li>

          <li><a href="index.html" accesskey="U">API Docs</a> &raquo;</li> 
      </ul>
    </div>

      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/logo.png" alt="Logo"/>
            </a></p>
            <h3><a href="../index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#"><tt class="docutils literal"><span class="pre">pydoop.hdfs</span></tt> &#8212; HDFS API</a><ul>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
<li><a class="reference internal" href="#pydoop-hdfs-path-path-name-manipulations">pydoop.hdfs.path &#8211; Path Name Manipulations</a></li>
<li><a class="reference internal" href="#pydoop-hdfs-fs-file-system-handles">pydoop.hdfs.fs &#8211; File System Handles</a></li>
<li><a class="reference internal" href="#pydoop-hdfs-file-hdfs-file-objects">pydoop.hdfs.file &#8211; HDFS File Objects</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="mr_api.html"
                                  title="previous chapter"><tt class="docutils literal"><span class="pre">pydoop.pipes</span></tt> &#8212; MapReduce API</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="utils.html"
                                  title="next chapter"><tt class="docutils literal"><span class="pre">pydoop.utils</span></tt> &#8212; Utility Functions</a></p>

					<h4>Get Pydoop</h4>
					<ul>
						<li> <a href="http://sourceforge.net/projects/pydoop/files/">Download page</a> </li>
						<li> <a href="../installation.html"> Installation Instructions </a> </li>
					</ul>

					<h4>Contributors</h4>
					<p class="topless">
					Pydoop is developed by:
					<a href="http://www.crs4.it">
						<img src="../_static/crs4.png" alt="CRS4" width="200" height="60" />
					</a>
					</p>

					And generously hosted by:
					<a href="http://sourceforge.net/projects/pydoop">
						<img src="http://sflogo.sourceforge.net/sflogo.php?group_id=536922&amp;type=13" width="120" height="30" 
						alt="Get Pydoop at SourceForge.net. Fast, secure and Free Open Source software downloads" />
					</a>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="../search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="module-pydoop.hdfs">
<span id="pydoop-hdfs-hdfs-api"></span><span id="hdfs-api"></span><h1><a class="reference internal" href="#module-pydoop.hdfs" title="pydoop.hdfs"><tt class="xref py py-mod docutils literal"><span class="pre">pydoop.hdfs</span></tt></a> &#8212; HDFS API<a class="headerlink" href="#module-pydoop.hdfs" title="Permalink to this headline">¶</a></h1>
<p>This module allows you to connect to an HDFS installation, read and
write files and get information on files, directories and global
filesystem properties.</p>
<div class="section" id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h2>
<p>The hdfs module is built on top of <tt class="docutils literal"><span class="pre">libhdfs</span></tt>, in turn a JNI wrapper
around the Java fs code: therefore, for the module to work properly,
the <tt class="docutils literal"><span class="pre">CLASSPATH</span></tt> environment variable must include all paths to the
relevant Hadoop jars. Pydoop will do this for you, but it needs to
know where your Hadoop installation is located and what is your hadoop
configuration directory: if Pydoop is not able to automatically find
these directories, you have to make sure that the <tt class="docutils literal"><span class="pre">HADOOP_HOME</span></tt> and
<tt class="docutils literal"><span class="pre">HADOOP_CONF_DIR</span></tt> environment variables are set to the appropriate
values.</p>
<p>Another important environment variable for this module is
<tt class="docutils literal"><span class="pre">LIBHDFS_OPTS</span></tt>. This is used to set options for the JVM on top of
which the module runs, most notably the amount of memory it uses. If
<tt class="docutils literal"><span class="pre">LIBHDFS_OPTS</span></tt> is not set, the C libhdfs will let it fall back to
the default for your system, typically 1 GB. According to our
experience, this is <em>much</em> more than most applications need and adds a
lot of unnecessary memory overhead. For this reason, the hdfs module
sets <tt class="docutils literal"><span class="pre">LIBHDFS_OPTS</span></tt> to <tt class="docutils literal"><span class="pre">-Xmx48m</span></tt>, a value that we found to be
appropriate for most applications.  If your needs are different, you
can set the environment variable externally and it will override the
above setting.</p>
<dl class="function">
<dt id="pydoop.hdfs.chmod">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">chmod</tt><big>(</big><em>hdfs_path</em>, <em>mode</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.chmod" title="Permalink to this definition">¶</a></dt>
<dd><p>Change file mode bits.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> (<em>string</em>) &#8211; the path to the file or directory</li>
<li><strong>mode</strong> (<em>int</em>) &#8211; the bitmask to set it to (e.g., 0777)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.cp">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">cp</tt><big>(</big><em>src_hdfs_path</em>, <em>dest_hdfs_path</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.cp" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy the contents of <tt class="docutils literal"><span class="pre">src_hdfs_path</span></tt> to <tt class="docutils literal"><span class="pre">dest_hdfs_path</span></tt>.</p>
<p>Additional keyword arguments, if any, are handled like in
<a class="reference internal" href="#pydoop.hdfs.open" title="pydoop.hdfs.open"><tt class="xref py py-func docutils literal"><span class="pre">open()</span></tt></a>.  If <tt class="docutils literal"><span class="pre">src_hdfs_path</span></tt> is a directory, its contents
will be copied recursively.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.dump">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">dump</tt><big>(</big><em>data</em>, <em>hdfs_path</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.dump" title="Permalink to this definition">¶</a></dt>
<dd><p>Write <tt class="docutils literal"><span class="pre">data</span></tt> to <tt class="docutils literal"><span class="pre">hdfs_path</span></tt>.</p>
<p>Additional keyword arguments, if any, are handled like in <a class="reference internal" href="#pydoop.hdfs.open" title="pydoop.hdfs.open"><tt class="xref py py-func docutils literal"><span class="pre">open()</span></tt></a>.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.get">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">get</tt><big>(</big><em>src_hdfs_path</em>, <em>dest_path</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy the contents of <tt class="docutils literal"><span class="pre">src_hdfs_path</span></tt> to <tt class="docutils literal"><span class="pre">dest_path</span></tt>.</p>
<p><tt class="docutils literal"><span class="pre">dest_path</span></tt> is forced to be interpreted as an ordinary local path
(see <a class="reference internal" href="#pydoop.hdfs.path.abspath" title="pydoop.hdfs.path.abspath"><tt class="xref py py-func docutils literal"><span class="pre">abspath()</span></tt></a>).  Additional keyword arguments, if any,
are handled like in <a class="reference internal" href="#pydoop.hdfs.open" title="pydoop.hdfs.open"><tt class="xref py py-func docutils literal"><span class="pre">open()</span></tt></a>.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.load">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">load</tt><big>(</big><em>hdfs_path</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Read the content of <tt class="docutils literal"><span class="pre">hdfs_path</span></tt> and return it.</p>
<p>Additional keyword arguments, if any, are handled like in <a class="reference internal" href="#pydoop.hdfs.open" title="pydoop.hdfs.open"><tt class="xref py py-func docutils literal"><span class="pre">open()</span></tt></a>.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.ls">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">ls</tt><big>(</big><em>hdfs_path</em>, <em>user=None</em>, <em>recursive=False</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.ls" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of hdfs paths.</p>
<p>Works in the same way as <a class="reference internal" href="#pydoop.hdfs.lsl" title="pydoop.hdfs.lsl"><tt class="xref py py-func docutils literal"><span class="pre">lsl()</span></tt></a>, except for the fact that list
items are hdfs paths instead of dictionaries of properties.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.lsl">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">lsl</tt><big>(</big><em>hdfs_path</em>, <em>user=None</em>, <em>recursive=False</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.lsl" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of dictionaries of file properties.</p>
<p>If <tt class="docutils literal"><span class="pre">hdfs_path</span></tt> is a file, there is only one item corresponding to
the file itself; if it is a directory and <tt class="docutils literal"><span class="pre">recursive</span></tt> is
<tt class="xref py py-obj docutils literal"><span class="pre">False</span></tt>, each list item corresponds to a file or directory
contained by it; if it is a directory and <tt class="docutils literal"><span class="pre">recursive</span></tt> is
<tt class="xref py py-obj docutils literal"><span class="pre">True</span></tt>, the list contains one item for every file or directory
in the tree rooted at <tt class="docutils literal"><span class="pre">hdfs_path</span></tt>.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.mkdir">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">mkdir</tt><big>(</big><em>hdfs_path</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.mkdir" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a directory and its parents as needed.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.move">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">move</tt><big>(</big><em>src</em>, <em>dest</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.move" title="Permalink to this definition">¶</a></dt>
<dd><p>Move or rename src to dest.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.open">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">open</tt><big>(</big><em>hdfs_path</em>, <em>mode='r'</em>, <em>buff_size=0</em>, <em>replication=0</em>, <em>blocksize=0</em>, <em>readline_chunk_size=16384</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.open" title="Permalink to this definition">¶</a></dt>
<dd><p>Open a file, returning an <tt class="xref py py-class docutils literal"><span class="pre">hdfs_file</span></tt> object.</p>
<p><tt class="docutils literal"><span class="pre">hdfs_path</span></tt> and <tt class="docutils literal"><span class="pre">user</span></tt> are passed to
<a class="reference internal" href="#pydoop.hdfs.path.split" title="pydoop.hdfs.path.split"><tt class="xref py py-func docutils literal"><span class="pre">split()</span></tt></a>, while the other args are
passed to the <tt class="xref py py-class docutils literal"><span class="pre">hdfs_file</span></tt> constructor.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.put">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">put</tt><big>(</big><em>src_path</em>, <em>dest_hdfs_path</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.put" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy the contents of <tt class="docutils literal"><span class="pre">src_path</span></tt> to <tt class="docutils literal"><span class="pre">dest_hdfs_path</span></tt>.</p>
<p><tt class="docutils literal"><span class="pre">src_path</span></tt> is forced to be interpreted as an ordinary local path
(see <a class="reference internal" href="#pydoop.hdfs.path.abspath" title="pydoop.hdfs.path.abspath"><tt class="xref py py-func docutils literal"><span class="pre">abspath()</span></tt></a>). Additional keyword arguments, if any,
are handled like in <a class="reference internal" href="#pydoop.hdfs.open" title="pydoop.hdfs.open"><tt class="xref py py-func docutils literal"><span class="pre">open()</span></tt></a>.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.rmr">
<tt class="descclassname">pydoop.hdfs.</tt><tt class="descname">rmr</tt><big>(</big><em>hdfs_path</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.rmr" title="Permalink to this definition">¶</a></dt>
<dd><p>Recursively remove files and directories.</p>
</dd></dl>

</div>
<span class="target" id="module-pydoop.hdfs.path"></span><div class="section" id="pydoop-hdfs-path-path-name-manipulations">
<h2>pydoop.hdfs.path &#8211; Path Name Manipulations<a class="headerlink" href="#pydoop-hdfs-path-path-name-manipulations" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="pydoop.hdfs.path.abspath">
<tt class="descclassname">pydoop.hdfs.path.</tt><tt class="descname">abspath</tt><big>(</big><em>hdfs_path</em>, <em>user=None</em>, <em>local=False</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.path.abspath" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an absolute path for <tt class="docutils literal"><span class="pre">hdfs_path</span></tt>.</p>
<p>The <tt class="docutils literal"><span class="pre">user</span></tt> arg is passed to <a class="reference internal" href="#pydoop.hdfs.path.split" title="pydoop.hdfs.path.split"><tt class="xref py py-func docutils literal"><span class="pre">split()</span></tt></a>. The <tt class="docutils literal"><span class="pre">local</span></tt> argument
forces <tt class="docutils literal"><span class="pre">hdfs_path</span></tt> to be interpreted as an ordinary local path:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s">&#39;/tmp&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pydoop.hdfs.path</span> <span class="kn">as</span> <span class="nn">hpath</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hpath</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s">&#39;file:/tmp&#39;</span><span class="p">)</span>
<span class="go">&#39;file:/tmp&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hpath</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s">&#39;file:/tmp&#39;</span><span class="p">,</span> <span class="n">local</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="go">&#39;file:/tmp/file:/tmp&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.path.basename">
<tt class="descclassname">pydoop.hdfs.path.</tt><tt class="descname">basename</tt><big>(</big><em>hdfs_path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.path.basename" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the final component of <tt class="docutils literal"><span class="pre">hdfs_path</span></tt>.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.path.dirname">
<tt class="descclassname">pydoop.hdfs.path.</tt><tt class="descname">dirname</tt><big>(</big><em>hdfs_path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.path.dirname" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the directory component of <tt class="docutils literal"><span class="pre">hdfs_path</span></tt>.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.path.exists">
<tt class="descclassname">pydoop.hdfs.path.</tt><tt class="descname">exists</tt><big>(</big><em>hdfs_path</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.path.exists" title="Permalink to this definition">¶</a></dt>
<dd><p>Return <tt class="docutils literal"><span class="pre">True</span></tt> if <tt class="docutils literal"><span class="pre">hdfs_path</span></tt> exists in the default HDFS, else <tt class="docutils literal"><span class="pre">False</span></tt>.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.path.isdir">
<tt class="descclassname">pydoop.hdfs.path.</tt><tt class="descname">isdir</tt><big>(</big><em>path</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.path.isdir" title="Permalink to this definition">¶</a></dt>
<dd><p>Return True if <tt class="docutils literal"><span class="pre">path</span></tt> refers to a directory; False otherwise.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.path.isfile">
<tt class="descclassname">pydoop.hdfs.path.</tt><tt class="descname">isfile</tt><big>(</big><em>path</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.path.isfile" title="Permalink to this definition">¶</a></dt>
<dd><p>Return True if <tt class="docutils literal"><span class="pre">path</span></tt> refers to a file; False otherwise.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.path.join">
<tt class="descclassname">pydoop.hdfs.path.</tt><tt class="descname">join</tt><big>(</big><em>*parts</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.path.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Join path name components, inserting <tt class="docutils literal"><span class="pre">/</span></tt> as needed.</p>
<p>If any component looks like an absolute path (i.e., it starts with
<tt class="docutils literal"><span class="pre">hdfs:</span></tt> or <tt class="docutils literal"><span class="pre">file:</span></tt>), all previous components will be discarded.</p>
<p>Note that this is <em>not</em> the reverse of <a class="reference internal" href="#pydoop.hdfs.path.split" title="pydoop.hdfs.path.split"><tt class="xref py py-func docutils literal"><span class="pre">split()</span></tt></a>, but rather a
specialized version of os.path.join. No check is made to determine
whether the returned string is a valid HDFS path.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.path.kind">
<tt class="descclassname">pydoop.hdfs.path.</tt><tt class="descname">kind</tt><big>(</big><em>path</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.path.kind" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the kind of item that the path references.</p>
<p>Return None if the path doesn&#8217;t exist.</p>
</dd></dl>

<dl class="function">
<dt id="pydoop.hdfs.path.split">
<tt class="descclassname">pydoop.hdfs.path.</tt><tt class="descname">split</tt><big>(</big><em>hdfs_path</em>, <em>user=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.path.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Split <tt class="docutils literal"><span class="pre">hdfs_path</span></tt> into a (hostname, port, path) tuple.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>hdfs_path</strong> (<em>string</em>) &#8211; an HDFS path, e.g., <tt class="docutils literal"><span class="pre">hdfs://localhost:9000/user/me</span></tt></li>
<li><strong>user</strong> (<em>string</em>) &#8211; user name used to resolve relative paths, defaults to the
current user</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">tuple</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">hostname, port, path</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<span class="target" id="module-pydoop.hdfs.fs"></span><div class="section" id="pydoop-hdfs-fs-file-system-handles">
<h2>pydoop.hdfs.fs &#8211; File System Handles<a class="headerlink" href="#pydoop-hdfs-fs-file-system-handles" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pydoop.hdfs.fs.hdfs">
<em class="property">class </em><tt class="descclassname">pydoop.hdfs.fs.</tt><tt class="descname">hdfs</tt><big>(</big><em>host='default'</em>, <em>port=0</em>, <em>user=None</em>, <em>groups=None</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs" title="Permalink to this definition">¶</a></dt>
<dd><p>A handle to an HDFS instance.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>host</strong> (<em>string</em>) &#8211; hostname or IP address of the HDFS NameNode. Set to an
empty string (and <tt class="docutils literal"><span class="pre">port</span></tt> to 0) to connect to the local file
system; set to <tt class="docutils literal"><span class="pre">'default'</span></tt> (and <tt class="docutils literal"><span class="pre">port</span></tt> to 0) to connect to the
default (i.e., the one defined in the Hadoop configuration files)
file system.</li>
<li><strong>port</strong> (<em>int</em>) &#8211; the port on which the NameNode is listening</li>
<li><strong>user</strong> (string or <tt class="docutils literal"><span class="pre">None</span></tt>) &#8211; the Hadoop domain user name. Defaults to the current
UNIX user. Note that, in MapReduce applications, since tasks are
spawned by the JobTracker, the default user will be the one that
started the JobTracker itself.</li>
<li><strong>groups</strong> (<em>list</em>) &#8211; ignored. Included for backwards compatibility.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> when connecting to the local file system, <tt class="docutils literal"><span class="pre">user</span></tt> is
ignored (i.e., it will always be the current UNIX user).</p>
<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.capacity">
<tt class="descname">capacity</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.capacity" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the raw capacity of the filesystem.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the raw capacity</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.chmod">
<tt class="descname">chmod</tt><big>(</big><em>path</em>, <em>mode</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.chmod" title="Permalink to this definition">¶</a></dt>
<dd><p>Change file mode bits.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>path</strong> (<em>string</em>) &#8211; the path to the file or directory</li>
<li><strong>mode</strong> (<em>int</em>) &#8211; the bitmask to set it to (e.g., 0777)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">IOError</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.chown">
<tt class="descname">chown</tt><big>(</big><em>path</em>, <em>user=''</em>, <em>group=''</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.chown" title="Permalink to this definition">¶</a></dt>
<dd><p>Change file owner and group.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>path</strong> (<em>string</em>) &#8211; the path to the file or directory</li>
<li><strong>user</strong> (<em>string</em>) &#8211; Hadoop username. Set to &#8216;&#8217; if only setting group</li>
<li><strong>group</strong> (<em>string</em>) &#8211; Hadoop group name. Set to &#8216;&#8217; if only setting user</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">IOError</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.close">
<tt class="descname">close</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.close" title="Permalink to this definition">¶</a></dt>
<dd><p>Close the HDFS handle (disconnect).</p>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.copy">
<tt class="descname">copy</tt><big>(</big><em>from_path</em>, <em>to_hdfs</em>, <em>to_path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy file from one filesystem to another.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>from_path</strong> (<em>string</em>) &#8211; the path of the source file</li>
<li><strong>to_hdfs</strong> &#8211; the handle to destination filesystem</li>
<li><strong>to_path</strong> (<em>string</em>) &#8211; the path of the destination file</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">IOError</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.create_directory">
<tt class="descname">create_directory</tt><big>(</big><em>path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.create_directory" title="Permalink to this definition">¶</a></dt>
<dd><p>Create directory <tt class="docutils literal"><span class="pre">path</span></tt> (non-existent parents will be created as well).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path</strong> (<em>string</em>) &#8211; the path of the directory</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body">IOError</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.default_block_size">
<tt class="descname">default_block_size</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.default_block_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the default block size.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the default blocksize</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.delete">
<tt class="descname">delete</tt><big>(</big><em>path</em>, <em>recursive=True</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.delete" title="Permalink to this definition">¶</a></dt>
<dd><p>Delete <tt class="docutils literal"><span class="pre">path</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>path</strong> (<em>string</em>) &#8211; the path of the file or directory</li>
<li><strong>recursive</strong> (<em>bool</em>) &#8211; if path is directory, delete it recursively when True;</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">IOError when <tt class="docutils literal"><span class="pre">recursive</span></tt> is False and directory is non-empty</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.exists">
<tt class="descname">exists</tt><big>(</big><em>path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.exists" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if a given path exists on the filesystem.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path</strong> (<em>string</em>) &#8211; the path to look for</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">bool</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">True if <tt class="docutils literal"><span class="pre">path</span></tt> exists, else False</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.get_hosts">
<tt class="descname">get_hosts</tt><big>(</big><em>path</em>, <em>start</em>, <em>length</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.get_hosts" title="Permalink to this definition">¶</a></dt>
<dd><p>Get hostnames where a particular block (determined by pos and
blocksize) of a file is stored. Due to replication, a single block
could be present on multiple hosts.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>path</strong> (<em>string</em>) &#8211; the path of the file</li>
<li><strong>start</strong> (<em>int</em>) &#8211; the start of the block</li>
<li><strong>length</strong> (<em>int</em>) &#8211; the length of the block</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">list</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">list of hosts that store the block</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.get_path_info">
<tt class="descname">get_path_info</tt><big>(</big><em>path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.get_path_info" title="Permalink to this definition">¶</a></dt>
<dd><p>Get information about <tt class="docutils literal"><span class="pre">path</span></tt> as a dict of properties.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path</strong> (<em>string</em>) &#8211; a path in the filesystem</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">dict</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">path information</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body">IOError</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydoop.hdfs.fs.hdfs.host">
<tt class="descname">host</tt><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.host" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual hdfs hostname (empty string for the local fs).</p>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.list_directory">
<tt class="descname">list_directory</tt><big>(</big><em>path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.list_directory" title="Permalink to this definition">¶</a></dt>
<dd><p>Get list of files and directories for <tt class="docutils literal"><span class="pre">path</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path</strong> (<em>string</em>) &#8211; the path of the directory</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">list of files and directories in <tt class="docutils literal"><span class="pre">path</span></tt></td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body">IOError</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.move">
<tt class="descname">move</tt><big>(</big><em>from_path</em>, <em>to_hdfs</em>, <em>to_path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.move" title="Permalink to this definition">¶</a></dt>
<dd><p>Move file from one filesystem to another.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>from_path</strong> (<em>string</em>) &#8211; the path of the source file</li>
<li><strong>to_hdfs</strong> &#8211; the handle to destination filesystem</li>
<li><strong>to_path</strong> (<em>string</em>) &#8211; the path of the destination file</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">IOError</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.open_file">
<tt class="descname">open_file</tt><big>(</big><em>path</em>, <em>flags=0</em>, <em>buff_size=0</em>, <em>replication=0</em>, <em>blocksize=0</em>, <em>readline_chunk_size=16384</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.open_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Open an HDFS file.</p>
<p>Pass 0 as buff_size, replication or blocksize if you want to use
the default values, i.e., the ones set in the Hadoop configuration
files.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>path</strong> (<em>string</em>) &#8211; the full path to the file</li>
<li><strong>flags</strong> (<em>string or int</em>) &#8211; opening flags: <tt class="docutils literal"><span class="pre">'r'</span></tt> or <tt class="xref py py-data docutils literal"><span class="pre">os.O_RDONLY</span></tt> for reading,
<tt class="docutils literal"><span class="pre">'w'</span></tt> or <tt class="xref py py-data docutils literal"><span class="pre">os.O_WRONLY</span></tt> for writing</li>
<li><strong>buff_size</strong> (<em>int</em>) &#8211; read/write buffer size in bytes</li>
<li><strong>replication</strong> (<em>int</em>) &#8211; HDFS block replication</li>
<li><strong>blocksize</strong> (<em>int</em>) &#8211; HDFS block size</li>
<li><strong>readline_chunk_size</strong> (<em>int</em>) &#8211; the amount of bytes that
<tt class="xref py py-meth docutils literal"><span class="pre">hdfs_file.readline()</span></tt> will use for buffering</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Rtpye:</th><td class="field-body"><p class="first"><tt class="xref py py-class docutils literal"><span class="pre">hdfs_file</span></tt></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">handle to the open file</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydoop.hdfs.fs.hdfs.port">
<tt class="descname">port</tt><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.port" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual hdfs port (0 for the local fs).</p>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.rename">
<tt class="descname">rename</tt><big>(</big><em>from_path</em>, <em>to_path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.rename" title="Permalink to this definition">¶</a></dt>
<dd><p>Rename file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>from_path</strong> (<em>string</em>) &#8211; the path of the source file</li>
<li><strong>to_path</strong> (<em>string</em>) &#8211; the path of the destination file</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">IOError</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.set_replication">
<tt class="descname">set_replication</tt><big>(</big><em>path</em>, <em>replication</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.set_replication" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the replication of <tt class="docutils literal"><span class="pre">path</span></tt> to <tt class="docutils literal"><span class="pre">replication</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>path</strong> (<em>string</em>) &#8211; the path of the file</li>
<li><strong>replication</strong> (<em>int</em>) &#8211; the replication value</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">IOError</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.set_working_directory">
<tt class="descname">set_working_directory</tt><big>(</big><em>path</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.set_working_directory" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the working directory to <tt class="docutils literal"><span class="pre">path</span></tt>. All relative paths will
be resolved relative to it.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path</strong> (<em>string</em>) &#8211; the path of the directory</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body">IOError</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.used">
<tt class="descname">used</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.used" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the total raw size of all files in the filesystem.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">total size of files in the file system</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydoop.hdfs.fs.hdfs.user">
<tt class="descname">user</tt><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.user" title="Permalink to this definition">¶</a></dt>
<dd><p>The user associated with this HDFS connection.</p>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.utime">
<tt class="descname">utime</tt><big>(</big><em>path</em>, <em>mtime</em>, <em>atime</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.utime" title="Permalink to this definition">¶</a></dt>
<dd><p>Change file last access and modification times.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>path</strong> (<em>string</em>) &#8211; the path to the file or directory</li>
<li><strong>mtime</strong> (<em>int</em>) &#8211; new modification time in seconds</li>
<li><strong>atime</strong> (<em>int</em>) &#8211; new access time in seconds</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">IOError</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.walk">
<tt class="descname">walk</tt><big>(</big><em>top</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.walk" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate infos for all paths in the tree rooted at <tt class="docutils literal"><span class="pre">top</span></tt> (included).</p>
<p>The <tt class="docutils literal"><span class="pre">top</span></tt> parameter can be either an HDFS path string or a
dictionary of properties as returned by <a class="reference internal" href="#pydoop.hdfs.fs.hdfs.get_path_info" title="pydoop.hdfs.fs.hdfs.get_path_info"><tt class="xref py py-meth docutils literal"><span class="pre">get_path_info()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>top</strong> (<em>string or dict</em>) &#8211; an HDFS path or path info dict</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">iterator</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">path infos of files and directories in the tree rooted at <tt class="docutils literal"><span class="pre">top</span></tt></td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body">IOError</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.fs.hdfs.working_directory">
<tt class="descname">working_directory</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.fs.hdfs.working_directory" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the current working directory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">str</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">current working directory</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-pydoop.hdfs.file"></span><div class="section" id="pydoop-hdfs-file-hdfs-file-objects">
<h2>pydoop.hdfs.file &#8211; HDFS File Objects<a class="headerlink" href="#pydoop-hdfs-file-hdfs-file-objects" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pydoop.hdfs.file.hdfs_file">
<em class="property">class </em><tt class="descclassname">pydoop.hdfs.file.</tt><tt class="descname">hdfs_file</tt><big>(</big><em>raw_hdfs_file</em>, <em>fs</em>, <em>name</em>, <em>flags</em>, <em>chunk_size=16384</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Instances of this class represent HDFS file objects.</p>
<p>Objects from this class should not be instantiated directly.  The
preferred way to open an HDFS file is with the <tt class="xref py py-func docutils literal"><span class="pre">open()</span></tt> function;
alternatively, <tt class="xref py py-meth docutils literal"><span class="pre">hdfs.open_file()</span></tt> can be used.</p>
<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.available">
<tt class="descname">available</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.available" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of bytes that can be read from this input stream without blocking.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">available bytes</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.close">
<tt class="descname">close</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.close" title="Permalink to this definition">¶</a></dt>
<dd><p>Close the file.</p>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.flush">
<tt class="descname">flush</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.flush" title="Permalink to this definition">¶</a></dt>
<dd><p>Force any buffered output to be written.</p>
</dd></dl>

<dl class="attribute">
<dt id="pydoop.hdfs.file.hdfs_file.fs">
<tt class="descname">fs</tt><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.fs" title="Permalink to this definition">¶</a></dt>
<dd><p>The file&#8217;s hdfs instance.</p>
</dd></dl>

<dl class="attribute">
<dt id="pydoop.hdfs.file.hdfs_file.mode">
<tt class="descname">mode</tt><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>The I/O mode for the file.</p>
</dd></dl>

<dl class="attribute">
<dt id="pydoop.hdfs.file.hdfs_file.name">
<tt class="descname">name</tt><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.name" title="Permalink to this definition">¶</a></dt>
<dd><p>The file&#8217;s fully qualified name.</p>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.next">
<tt class="descname">next</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.next" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the next input line, or raise <tt class="xref py py-class docutils literal"><span class="pre">StopIteration</span></tt>
when EOF is hit.</p>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.pread">
<tt class="descname">pread</tt><big>(</big><em>position</em>, <em>length</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.pread" title="Permalink to this definition">¶</a></dt>
<dd><p>Read <tt class="docutils literal"><span class="pre">length</span></tt> bytes of data from the file, starting from <tt class="docutils literal"><span class="pre">position</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>position</strong> (<em>int</em>) &#8211; position from which to read</li>
<li><strong>length</strong> (<em>int</em>) &#8211; the number of bytes to read</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">string</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">the chunk of data read from the file</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.pread_chunk">
<tt class="descname">pread_chunk</tt><big>(</big><em>position</em>, <em>chunk</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.pread_chunk" title="Permalink to this definition">¶</a></dt>
<dd><p>Works like <a class="reference internal" href="#pydoop.hdfs.file.hdfs_file.pread" title="pydoop.hdfs.file.hdfs_file.pread"><tt class="xref py py-meth docutils literal"><span class="pre">pread()</span></tt></a>, but data is stored in the writable
buffer <tt class="docutils literal"><span class="pre">chunk</span></tt> rather than returned. Reads at most a number of
bytes equal to the size of <tt class="docutils literal"><span class="pre">chunk</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>position</strong> (<em>int</em>) &#8211; position from which to read</li>
<li><strong>chunk</strong> (<em>writable string buffer</em>) &#8211; a c-like string buffer, such as the one returned by the
<tt class="docutils literal"><span class="pre">create_string_buffer</span></tt> function in the <tt class="xref py py-mod docutils literal"><span class="pre">ctypes</span></tt> module</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">int</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">the number of bytes read</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.read">
<tt class="descname">read</tt><big>(</big><em>length=-1</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Read <tt class="docutils literal"><span class="pre">length</span></tt> bytes from the file.  If <tt class="docutils literal"><span class="pre">length</span></tt> is negative or
omitted, read all data until EOF.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>length</strong> (<em>int</em>) &#8211; the number of bytes to read</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">string</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the chunk of data read from the file</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.read_chunk">
<tt class="descname">read_chunk</tt><big>(</big><em>chunk</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.read_chunk" title="Permalink to this definition">¶</a></dt>
<dd><p>Works like <a class="reference internal" href="#pydoop.hdfs.file.hdfs_file.read" title="pydoop.hdfs.file.hdfs_file.read"><tt class="xref py py-meth docutils literal"><span class="pre">read()</span></tt></a>, but data is stored in the writable
buffer <tt class="docutils literal"><span class="pre">chunk</span></tt> rather than returned. Reads at most a number of
bytes equal to the size of <tt class="docutils literal"><span class="pre">chunk</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>chunk</strong> (<em>writable string buffer</em>) &#8211; a c-like string buffer, such as the one returned by the
<tt class="docutils literal"><span class="pre">create_string_buffer</span></tt> function in the <tt class="xref py py-mod docutils literal"><span class="pre">ctypes</span></tt> module</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the number of bytes read</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.readline">
<tt class="descname">readline</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.readline" title="Permalink to this definition">¶</a></dt>
<dd><p>Read and return a line of text.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">string</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the next line of text in the file, including the newline character</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.seek">
<tt class="descname">seek</tt><big>(</big><em>position</em>, <em>whence=0</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.seek" title="Permalink to this definition">¶</a></dt>
<dd><p>Seek to <tt class="docutils literal"><span class="pre">position</span></tt> in file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>position</strong> (<em>int</em>) &#8211; offset in bytes to seek to</li>
<li><strong>whence</strong> (<em>int</em>) &#8211; defaults to <tt class="docutils literal"><span class="pre">os.SEEK_SET</span></tt> (absolute); other
values are <tt class="docutils literal"><span class="pre">os.SEEK_CUR</span></tt> (relative to the current position)
and <tt class="docutils literal"><span class="pre">os.SEEK_END</span></tt> (relative to the file&#8217;s end).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pydoop.hdfs.file.hdfs_file.size">
<tt class="descname">size</tt><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.size" title="Permalink to this definition">¶</a></dt>
<dd><p>The file&#8217;s size in bytes. This attribute is initialized when the
file is opened and updated when it is closed.</p>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.tell">
<tt class="descname">tell</tt><big>(</big><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.tell" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the current byte offset in the file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">current offset in bytes</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.write">
<tt class="descname">write</tt><big>(</big><em>data</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.write" title="Permalink to this definition">¶</a></dt>
<dd><p>Write <tt class="docutils literal"><span class="pre">data</span></tt> to the file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>data</strong> (<em>string</em>) &#8211; the data to be written to the file</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the number of bytes written</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pydoop.hdfs.file.hdfs_file.write_chunk">
<tt class="descname">write_chunk</tt><big>(</big><em>chunk</em><big>)</big><a class="headerlink" href="#pydoop.hdfs.file.hdfs_file.write_chunk" title="Permalink to this definition">¶</a></dt>
<dd><p>Write data from buffer <tt class="docutils literal"><span class="pre">chunk</span></tt> to the file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>chunk</strong> (<em>writable string buffer</em>) &#8211; a c-like string buffer, such as the one returned by the
<tt class="docutils literal"><span class="pre">create_string_buffer</span></tt> function in the <tt class="xref py py-mod docutils literal"><span class="pre">ctypes</span></tt> module</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the number of bytes written</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="utils.html" title="pydoop.utils — Utility Functions"
             >next</a> |</li>
        <li class="right" >
          <a href="mr_api.html" title="pydoop.pipes — MapReduce API"
             >previous</a> |</li>
	<li><a href="../index.html">Home</a>|&nbsp;</li>
	<li><a href="../installation.html">Download & Install</a>|&nbsp;</li>
	<li><a href="https://sourceforge.net/projects/pydoop/forums/forum/990018">Support</a>|&nbsp;</li>
	<li><a href="http://sourceforge.net/projects/pydoop/">Pydoop on SF</a></li>

          <li><a href="index.html" >API Docs</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2009-2014, CRS4.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>