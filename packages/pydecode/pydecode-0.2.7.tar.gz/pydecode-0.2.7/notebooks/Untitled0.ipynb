{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "h1:\n",
      "Tutorial 5: Training a CRF\n",
      "\n",
      "In [2]:\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from collections import namedtuple\n",
      "import pydecode.model as model\n",
      "import pydecode.chart as chart\n",
      "\n",
      "from collections import Counter, defaultdict\n",
      "from itertools import izip\n",
      "\n",
      "In [3]:\n",
      "class Dictionary:\n",
      "    def __init__(self, counts, word_counts, tag_set):\n",
      "        self.counts = counts \n",
      "        self.word_counts = word_counts\n",
      "        self.tag_set = tag_set\n",
      "        self.tag_num = {tag:i for i, tag in enumerate(tag_set)}\n",
      "        self.word_num = {word:i for i, word in enumerate(word_counts.iterkeys())}\n",
      "\n",
      "    def emission(self, word):\n",
      "        if word == \"ROOT\": return [\"<t>\"]\n",
      "        if word == \"END\": return [\"</t>\"]\n",
      "        if self.word_counts[word] > 5:\n",
      "            return self.counts[word].keys()\n",
      "        return self.tag_set\n",
      "\n",
      "    def tag_id(self, tag):\n",
      "        return self.tag_num.get(tag, 0)\n",
      "\n",
      "    def word_id(self, word):\n",
      "        return self.word_num.get(word, 0)\n",
      "\n",
      "    @staticmethod\n",
      "    def make(sentences, taggings):\n",
      "        tag_set = set()\n",
      "        word_counts = Counter()\n",
      "        counts = defaultdict(Counter)\n",
      "        for sentence, tags in izip(sentences, taggings):\n",
      "            #print sentence, tags\n",
      "            for word, tag in izip(sentence, tags):\n",
      "                counts[word][tag.tag] += 1\n",
      "                word_counts[word] += 1\n",
      "                tag_set.add(tag.tag)\n",
      "        print tag_set\n",
      "        return Dictionary(counts, word_counts, tag_set)\n",
      "\n",
      "    \n",
      "\n",
      "In [4]:\n",
      "class Bigram(namedtuple(\"Bigram\", [\"position\", \"prevtag\", \"tag\"])):\n",
      "    def __str__(self): return \"%s -> %s\"%(self.prevtag, self.tag)\n",
      "    \n",
      "    @staticmethod\n",
      "    def from_tagging(tagging):\n",
      "        return [Bigram(i, tag=tag, prevtag=tagging[i-1] if i > 0 else \"<t>\")\n",
      "                for i, tag in enumerate(tagging)] + [Bigram(len(tagging), tag=\"</t>\", prevtag=tagging[-1])] \n",
      "      \n",
      "class Tagged(namedtuple(\"Tagged\", [\"position\",  \"tag\"])):\n",
      "    def __str__(self): return \"%s\"%(self.tag,)\n",
      "\n",
      "In [16]:\n",
      "class TaggingCRFModel(model.DynamicProgrammingModel):\n",
      "    def initialize(self, sentences, tags):\n",
      "        self.dictionary = Dictionary.make(sentences, tags)\n",
      "        super(TaggingCRFModel, self).initialize(sentences, tags)\n",
      "\n",
      "\n",
      "    def dynamic_program(self, sentence, c):\n",
      "        words = [\"ROOT\"] + sentence + [\"END\"]\n",
      "        c.init(Tagged(0, \"<t>\"))\n",
      "        for i, word in enumerate(words[1:], 1):\n",
      "            prev_tags = self.dictionary.emission(words[i-1])\n",
      "            for tag in self.dictionary.emission(word):\n",
      "                c[Tagged(i, tag)] = \\\n",
      "                    c.sum([c[key] * c.sr(Bigram(i - 1, prev, tag))\n",
      "                           for prev in prev_tags \n",
      "                           for key in [Tagged(i - 1, prev)] \n",
      "                           if key in c])\n",
      "        return c\n",
      "\n",
      "    def initialize_features(self, sentence):\n",
      "        return [self.dictionary.word_id(word) for word in sentence]\n",
      "\n",
      "    def factored_psi(self, sentence, bigram, data):\n",
      "        word = sentence[bigram.position] if bigram.position < len(sentence) else \"END\"\n",
      "        return {\"word:tag:%s:%s\" % (bigram.tag, word) : 1, \n",
      "                \"suff:word:tag:%d:%s:%s\" % (1, bigram.tag, word[-1:]) : 1, \n",
      "                \"suff:word:tag:%d:%s:%s\" % (2, bigram.tag, word[-2:]) : 1, \n",
      "                \"suff:word:tag:%d:%s:%s\" % (3, bigram.tag, word[-3:]) : 1, \n",
      "                \"pre:word:tag:%d:%s:%s\" % (1, bigram.tag, word[:1]) : 1, \n",
      "                \"pre:word:tag:%d:%s:%s\" % (2, bigram.tag, word[:2]) : 1, \n",
      "                \"pre:word:tag:%d:%s:%s\" % (3, bigram.tag, word[:3]) : 1, \n",
      "                \"word:%s\" %  word : 1, \n",
      "                \"tag-1:%s\" % bigram.prevtag : 1, \n",
      "                \"tag:%s\" % bigram.tag : 1,\n",
      "                \"bi:%s:%s\" % (bigram.prevtag, bigram.tag): 1,\n",
      "                }\n",
      "\n",
      "In [14]:\n",
      "data_X = map(lambda a: a.split(),\n",
      "             [\"the dog walked\",\n",
      "              \"in the park\",\n",
      "              \"in the dog\"])\n",
      "data_Y = map(lambda a: Bigram.from_tagging(a.split()),\n",
      "             [\"D N V\", \"I D N\", \"I D N\"])\n",
      "\n",
      "In [17]:\n",
      "def parse_training(handle):\n",
      "    x = []\n",
      "    y = []\n",
      "    for l in handle:\n",
      "        if not l.strip():\n",
      "            yield (x, y)\n",
      "            x = []\n",
      "            y = []\n",
      "        else:\n",
      "            word, tag = l.split()\n",
      "            x.append(word)\n",
      "            y.append(tag)\n",
      "    yield (x, y)\n",
      "data_X, data_Y = zip(*parse_training(open(\"tag/tag_train_small.dat\")))\n",
      "data_Y = [Bigram.from_tagging(t) for t in data_Y] \n",
      "\n",
      "In [245]:\n",
      "hm = TaggingCRFModel()\n",
      "hm.initialize(data_X, data_Y)\n",
      "for i in range(len(data_X))[:10]:\n",
      "    s = set(data_Y[i])\n",
      "    c = chart.ChartBuilder(lambda a: a,\n",
      "                           chart.HypergraphSemiRing, True)\n",
      "    hm.dynamic_program(data_X[i], c)\n",
      "    h = c.finish()\n",
      "    bool_pot = ph.BoolPotentials(h).build(lambda a: a in s)\n",
      "    path = ph.best_path(h, bool_pot)\n",
      "    #for edge in path: print h.label(edge)\n",
      "    assert bool_pot.dot(path)\n",
      "set(['ADV', 'NOUN', 'ADP', 'PRT', 'DET', '.', 'PRON', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ'])\n",
      "\n",
      "\n",
      "In [205]:\n",
      "print data_Y[0]\n",
      "[Bigram(position=0, prevtag='<t>', tag='NOUN'), Bigram(position=1, prevtag='NOUN', tag='NOUN'), Bigram(position=2, prevtag='NOUN', tag='VERB'), Bigram(position=3, prevtag='VERB', tag='NOUN'), Bigram(position=4, prevtag='NOUN', tag='.'), Bigram(position=5, prevtag='.', tag='</t>')]\n",
      "\n",
      "\n",
      "In [18]:\n",
      "from pystruct.learners import StructuredPerceptron\n",
      "hm = TaggingCRFModel()\n",
      "sp = StructuredPerceptron(hm, verbose=1, max_iter=25)\n",
      "\n",
      "with warnings.catch_warnings():\n",
      "    warnings.simplefilter(\"ignore\")\n",
      "    sp.fit(data_X, data_Y)\n",
      "\n",
      "set(['ADV', 'NOUN', 'ADP', 'PRT', 'DET', '.', 'PRON', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ'])\n",
      "iteration 0\n",
      "avg loss: 0.320848 w: [[ 1.  1. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 1\n",
      "avg loss: 0.195871 w: [[ 2.  2. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 2\n",
      "avg loss: 0.152933 w: [[ 1.  2.  0. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 3\n",
      "avg loss: 0.128814 w: [[ 1.  2.  0. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 4\n",
      "avg loss: 0.105609 w: [[ 2.  2.  0. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 5\n",
      "avg loss: 0.089530 w: [[ 1.  2. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 6\n",
      "avg loss: 0.082222 w: [[ 1.  3.  0. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 7\n",
      "avg loss: 0.081856 w: [[ 1.  3. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 8\n",
      "avg loss: 0.068884 w: [[ 1.  3. -2. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 9\n",
      "avg loss: 0.061940 w: [[ 1.  3. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 10\n",
      "avg loss: 0.046410 w: [[ 1.  3. -2. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 11\n",
      "avg loss: 0.052257 w: [[ 0.  3. -2. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 12\n",
      "avg loss: 0.049881 w: [[ 0.  3. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 13\n",
      "avg loss: 0.040563 w: [[ 1.  3. -3. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 14\n",
      "avg loss: 0.041476 w: [[ 1.  3. -3. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 15\n",
      "avg loss: 0.044034 w: [[ 1.  3. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 16\n",
      "avg loss: 0.032523 w: [[ 1.  3.  0. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 17\n",
      "avg loss: 0.037274 w: [[ 1.  3. -2. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 18\n",
      "avg loss: 0.036543 w: [[ 1.  3. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 19\n",
      "avg loss: 0.034350 w: [[ 1.  3. -2. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 20\n",
      "avg loss: 0.034899 w: [[ 1.  3. -2. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 21\n",
      "avg loss: 0.029783 w: [[ 1.  3. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 22\n",
      "avg loss: 0.034716 w: [[ 1.  3. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 23\n",
      "avg loss: 0.028138 w: [[ 1.  3. -1. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "iteration 24\n",
      "avg loss: 0.029417 w: [[ 0.  3. -2. ...,  0.  0.  0.]]\n",
      "effective learning rate: 1.000000\n",
      "\n",
      "\n",
      "In [20]:\n",
      "\n",
      "words = \"Ms. Haag plays Elianti .\".split()\n",
      "sp.predict([words])\n",
      "Out [20]:\n",
      "[{Bigram(position=0, prevtag='<t>', tag='NOUN'),\n",
      "  Bigram(position=1, prevtag='NOUN', tag='NOUN'),\n",
      "  Bigram(position=2, prevtag='NOUN', tag='VERB'),\n",
      "  Bigram(position=3, prevtag='VERB', tag='NOUN'),\n",
      "  Bigram(position=4, prevtag='NOUN', tag='.'),\n",
      "  Bigram(position=5, prevtag='.', tag='</t>')}]\n",
      "\n",
      "In [200]:\n",
      "c = Counter()\n",
      "c[\"ell\"] += 20\n",
      "c.keys()\n",
      "Out [200]:\n",
      "['ell']\n",
      "\n",
      "In [201]:\n",
      "# from  pystruct.plot_learning import plot_learning\n",
      "# plot_learning(sp)\n",
      "\n"
     ],
     "language": "python",
     "outputs": []
    }
   ]
  }
 ]
}